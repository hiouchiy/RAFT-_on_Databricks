{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab93ba68-0a3c-4fa6-87cb-92ee8546d471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 環境：サーバーレス・ノートブック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "504298e2-4554-4baa-8a1a-0f03bd07bb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raft-dataset/examples/raft_dataset.ipynb\n",
    "\n",
    "# RAFT Dataset LlamaPack\n",
    "\n",
    "このLlamaPackはRAFT: Adapting Language Model to Domain Specific RAG [論文](https://arxiv.org/abs/2403.10131)を実装しています。\n",
    "\n",
    "Retrieval Augmented FineTuning (RAFT)は、この論文で紹介されている学習レシピで、オープンブック、ドメイン内の質問応答タスクにおける大規模言語モデル(LLM)の性能を向上させることを目的としている。RAFTは、質問と検索された文書セットが与えられたとき、LLMが、無関係な情報や散漫な情報を無視しながら、質問の答えに役立つ文書から最も関連性の高いシーケンスを特定し、逐語的に引用するように訓練する。RAFTは、関連する情報と関連しない情報を区別し、関連する文書から証拠を提供するようにモデルを明示的に訓練することで、LLMがより優れた推論と説明の能力を開発することを促し、最終的には、追加のコンテキストや知識が利用可能なシナリオで質問に正確かつ合理的に回答する能力を向上させる。\n",
    "\n",
    "RAFTの重要な構成要素は、微調整のためのデータセットの生成方法である。各QAペアには、質問の答えを推測できる「オラクル」文書と、無関係な「ディストラクター」文書も含まれる。学習中、これはモデルにどの情報が関連/非関連かを学習させ、ドメイン知識も記憶させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79f16630-b9c4-4ea4-a82e-fb209313a5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445c9469-47eb-4c31-904b-e9a0c8f0e7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-packs-raft-dataset llama-index-embeddings-databricks llama-index-llms-databricks\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bcde2f8-efe1-4b0d-8817-0b8ef84af5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f07cf3-1986-4c68-af87-f3a6ef3758d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!wget --user-agent \"Mozilla\" \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -O './paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb0e474-4098-46d8-b063-900199800263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above dataset is HuggingFace Dataset format. You can then save it into `.arrow` or `.jsonl` format and use it for further finetuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7f8e931-bc9b-42a9-a34f-12d54b4ec2fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### You can refer to the original implementation [here](https://github.com/ShishirPatil/gorilla/tree/main/raft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e309942-e45d-4c88-be36-dfcf379222f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raft-dataset/llama_index/packs/raft_dataset/base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c77b4e-c81e-464d-8ef3-633f48f9147a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"RAFT Dataset LlamaPack class.\"\"\"\n",
    "\n",
    "# Inspired from https://github.com/ShishirPatil/gorilla/tree/main/raft\n",
    "\n",
    "from typing import Any, List\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configure logging to output to the console, with messages of level DEBUG and above\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from llama_index.core.llama_pack.base import BaseLlamaPack\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 512\n",
    "DEFAULT_BREAKPOINT_PERCENTILE_THRESHOLD = 95\n",
    "\n",
    "\n",
    "class MyRAFTDatasetPack(BaseLlamaPack):\n",
    "    \"\"\"RAFT Dataset Generator pack.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        llm: Any = None,\n",
    "        embed_model: Any = None,\n",
    "        num_questions_per_chunk: int = 5,\n",
    "        num_distract_docs: int = 3,\n",
    "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "        default_breakpoint_percentile_threshold=DEFAULT_BREAKPOINT_PERCENTILE_THRESHOLD,\n",
    "    ):\n",
    "        self.file_path = file_path\n",
    "        self.num_questions_per_chunk = num_questions_per_chunk\n",
    "        self.num_distract_docs = num_distract_docs\n",
    "        self.chunk_size = chunk_size\n",
    "        self.default_breakpoint_percentile_threshold = (\n",
    "            default_breakpoint_percentile_threshold\n",
    "        )\n",
    "        self.ds = None\n",
    "        self.llm = OpenAI(temperature=0, n=1, model=\"gpt-4o\") if llm is None else llm\n",
    "        self.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\") if embed_model is None else embed_model\n",
    "\n",
    "    def strip_str(self, s) -> str:\n",
    "        \"\"\"\n",
    "        Helper function for helping format strings returned by GPT-4.\n",
    "        \"\"\"\n",
    "        if s.startswith(\"assistant:\"):  # Check if the string starts with 'assistant '\n",
    "            s = s.replace(\"assistant:\", \"\", 1)  # Replace the first occurrence\n",
    "\n",
    "        start_index, end_index = 0, len(s) - 1\n",
    "        beg_found = False\n",
    "        for i in range(len(s)):\n",
    "            if s[i].isalpha():\n",
    "                if not beg_found:\n",
    "                    start_index = i\n",
    "                    beg_found = True\n",
    "                else:\n",
    "                    end_index = i\n",
    "        end_index += 2\n",
    "        return s[start_index : min(end_index, len(s))]\n",
    "\n",
    "    def encode_question_gen(self, question, chunk) -> List[str]:\n",
    "        \"\"\"\n",
    "        Encode multiple prompt instructions into a single string for the general case.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "            Question: {question}\\nContext: {chunk}\\n\n",
    "            Answer this question using the information given in the context above. Here is things to pay attention to:\n",
    "            - First provide step-by-step reasoning in Japanese on how to answer the question.\n",
    "            - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context.\n",
    "            - End your response with final answer in the form <ANSWER>: $answer, the answer should be succinct.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=\"You are a helpful question answerer who can provide an answer in Japanese given a question and relevant context.\",\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=prompt),\n",
    "        ]\n",
    "\n",
    "    def translate_doc_gen(self, chunk) -> str:\n",
    "        \"\"\"\n",
    "        Encode multiple prompt instructions into a single string for the general case.\n",
    "        \"\"\"\n",
    "        prompt = chunk\n",
    "        question_messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=\"\"\"You are an excellent English-Japanese interpreter. Translate the given English text into Japanese.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "Output only the translated Japanese text as a single, cohesive paragraph.\"\"\",),\n",
    "            ChatMessage(role=\"user\", content=prompt),\n",
    "        ]\n",
    "\n",
    "        response = self.llm.chat(question_messages)\n",
    "        return str(response)\n",
    "    \n",
    "    def rewrite_doc_gen(self, chunk) -> str:\n",
    "        \"\"\"\n",
    "        Encode multiple prompt instructions into a single string for the general case.\n",
    "        \"\"\"\n",
    "        prompt = chunk\n",
    "        question_messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=\"\"\"You are an excellent Japanese writer. Given text is not natural Japanese because it is translated from English by a non-native Japanese interpreter, so you have to rewrite it for more natural Japanese. 自分の信じて限界を超えて下さい！\n",
    "\n",
    "# Output Format\n",
    "\n",
    "Output only the rewrited Japanese text as a single, cohesive paragraph.\"\"\",),\n",
    "            ChatMessage(role=\"user\", content=prompt),\n",
    "        ]\n",
    "\n",
    "        response = self.llm.chat(question_messages)\n",
    "        return str(response)\n",
    "    \n",
    "    def generate_label(self, question, context) -> str:\n",
    "        \"\"\"\n",
    "        Generates the label / answer to `question` using `context` and GPT-4.\n",
    "        \"\"\"\n",
    "        question_messages = self.encode_question_gen(question, context)\n",
    "        response = self.llm.chat(question_messages)\n",
    "        return str(response)\n",
    "\n",
    "    def generate_instructions_gen(self, chunk, x=5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates `x` questions / use cases for `chunk`. Used when the input document is of general types\n",
    "        `pdf`, `json`, or `txt`.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=\"You are a synthetic question-answer pair generator. Given a chunk of context about some topic(s), generate %s example questions in Japanese which a user could ask and would be answered using information from the chunk. For example, if the given context was a Wikipedia paragraph about the United States, an example question could be 'アメリカにはいくつの州がありますか？'. The questions should be able to be answered in a few words or less.\"\n",
    "                % (x),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=str(chunk)),\n",
    "        ]\n",
    "\n",
    "        queries = str(self.llm.chat(messages)).split(\"\\n\")\n",
    "        questions = [self.strip_str(q) for q in queries]\n",
    "        questions = [q for q in questions if any(c.isalpha() for c in q)][:x]\n",
    "\n",
    "        num_questions_generated = len(questions)\n",
    "        if num_questions_generated < x:\n",
    "            warnings.warn(\n",
    "                f\"Fewer questions generated ({num_questions_generated}) \"\n",
    "                f\"than requested ({x}).\"\n",
    "            )\n",
    "\n",
    "        return questions\n",
    "\n",
    "    def get_chunks(self, file_path: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Takes in a `file_path`, retrieves the document, breaks it down into chunks of size\n",
    "        `chunk_size`, and returns the chunks.\n",
    "        \"\"\"\n",
    "        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "        splitter = SemanticSplitterNodeParser(\n",
    "            buffer_size=1,\n",
    "            breakpoint_percentile_threshold=self.default_breakpoint_percentile_threshold,\n",
    "            embed_model=self.embed_model,\n",
    "        )\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "        chunks = []\n",
    "        for node in nodes:\n",
    "            translated_text = self.translate_doc_gen(node.get_content())\n",
    "            refined_text = self.rewrite_doc_gen(translated_text)\n",
    "            chunks.append(refined_text)\n",
    "            print(refined_text)\n",
    "            print(\"--------------------------------------------------------------\")\n",
    "\n",
    "        return chunks #[self.translate_doc_gen(node.get_content()) for node in nodes]\n",
    "\n",
    "    def add_chunk_to_dataset(\n",
    "        self,\n",
    "        chunks: List,\n",
    "        chunk: str,\n",
    "        x: int = 5,\n",
    "        num_distract: int = 3,\n",
    "        p: float = 1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Given a chunk, create {Q, A, D} triplets and add them to the dataset.\n",
    "        \"\"\"\n",
    "        i = chunks.index(chunk)\n",
    "        qs = self.generate_instructions_gen(chunk, x)\n",
    "        for q in qs:\n",
    "            datapt = {\n",
    "                \"id\": None,\n",
    "                \"type\": None,\n",
    "                \"question\": None,\n",
    "                \"context\": None,\n",
    "                \"oracle_context\": None,\n",
    "                \"cot_answer\": None,\n",
    "            }\n",
    "\n",
    "            datapt[\"id\"] = f\"seed_task_{0 if not self.ds else self.ds.num_rows}\"\n",
    "            datapt[\"type\"] = \"general\"\n",
    "            datapt[\"question\"] = q\n",
    "\n",
    "            # add distractor docs\n",
    "            docs = [chunk]\n",
    "            indices = list(range(len(chunks)))\n",
    "            indices.remove(i)\n",
    "            for j in random.sample(indices, num_distract):\n",
    "                docs.append(chunks[j])\n",
    "            # decides whether to add oracle document\n",
    "            oracle = random.uniform(0, 1) < p\n",
    "            if not oracle:\n",
    "                docs[0] = chunks[random.sample(indices, 1)[0]]\n",
    "            random.shuffle(docs)\n",
    "\n",
    "            d = {\"title\": [], \"sentences\": []}\n",
    "\n",
    "            d[\"title\"].append([\"placeholder_title\"] * (num_distract + 1))\n",
    "            d[\"sentences\"].append(docs)\n",
    "            datapt[\"context\"] = d\n",
    "            datapt[\"oracle_context\"] = chunk\n",
    "\n",
    "            # add answer to q\n",
    "            datapt[\"cot_answer\"] = self.generate_label(q, chunk)\n",
    "\n",
    "            # construct model instruction\n",
    "            context = \"\"\n",
    "            for doc in docs:\n",
    "                context += \"<DOCUMENT>\" + str(doc) + \"</DOCUMENT>\\n\"\n",
    "            context += q\n",
    "            datapt[\"instruction\"] = context\n",
    "\n",
    "            # add to dataset\n",
    "            if not self.ds:\n",
    "                # init ds\n",
    "                datapt[\"id\"] = [datapt[\"id\"]]\n",
    "                datapt[\"type\"] = [datapt[\"type\"]]\n",
    "                datapt[\"question\"] = [datapt[\"question\"]]\n",
    "                datapt[\"context\"] = [datapt[\"context\"]]\n",
    "                datapt[\"oracle_context\"] = [datapt[\"oracle_context\"]]\n",
    "                datapt[\"cot_answer\"] = [datapt[\"cot_answer\"]]\n",
    "                datapt[\"instruction\"] = [datapt[\"instruction\"]]\n",
    "                self.ds = Dataset.from_dict(datapt)\n",
    "            else:\n",
    "                self.ds = self.ds.add_item(datapt)\n",
    "\n",
    "    def run(self) -> Any:\n",
    "        \"\"\"Run the pipeline.\"\"\"\n",
    "        chunks = self.get_chunks(self.file_path, self.chunk_size)\n",
    "\n",
    "        logger.info(f\"Number of chunks created: {len(chunks)}\")\n",
    "\n",
    "        self.num_distract_docs = (\n",
    "            min(self.num_distract_docs, len(chunks)) - 1\n",
    "        )  # should be less than number of chunks/ nodes created\n",
    "\n",
    "        for index, chunk in enumerate(chunks):\n",
    "            logger.info(f\"Processing chunk: {index}\")\n",
    "            self.add_chunk_to_dataset(\n",
    "                chunks, chunk, self.num_questions_per_chunk, self.num_distract_docs\n",
    "            )\n",
    "\n",
    "        return self.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32864427-5cea-4d96-94f7-cfc1a2bb1eca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks サービングエンドポイントをルーティングするために使用される Databricks URL とトークン\n",
    "databricks_host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "databricks_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3277639f-daa0-48d8-a995-5d5b5feac494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EMB_MODEL_NAME = \"YOUR-EMBEDDING-MODEL-NAME\"\n",
    "LLM_MODEL_NAME = \"YOUR-LLM-MODEL-NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dbd40fb-07ec-414d-8745-ce71667b94fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.databricks import DatabricksEmbedding\n",
    "\n",
    "# Set up the DatabricksEmbedding class with the required model, API key and serving endpoint\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = databricks_token\n",
    "os.environ[\"DATABRICKS_SERVING_ENDPOINT\"] = f\"{databricks_host}/serving-endpoints\"\n",
    "embed_model = DatabricksEmbedding(model=EMB_MODEL_NAME)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "\n",
    "# Embed some text\n",
    "embeddings = embed_model.get_text_embedding(\n",
    "    \"The DatabricksEmbedding integration works great.\"\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f164240-3928-4d16-9c83-67748f4f5d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.databricks import Databricks\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Databricks(\n",
    "    model=LLM_MODEL_NAME,\n",
    "    api_key=databricks_token,\n",
    "    api_base=f\"{databricks_host}/serving-endpoints\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb44e443-d741-4232-a16f-1f3b90de2d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raft_dataset = MyRAFTDatasetPack(\n",
    "  \"./paul_graham_essay.txt\", \n",
    "  llm=llm, \n",
    "  embed_model=embed_model)\n",
    "  \n",
    "dataset = raft_dataset.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a05eb46-f7f8-4aaf-8af4-05863a7da660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"raft_training_dataset_ja\"\n",
    "\n",
    "# Save as .arrow format\n",
    "dataset.save_to_disk(output_path)\n",
    "\n",
    "# Save as .jsonl format\n",
    "dataset.to_json(output_path + \".jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6db4b6d-cc70-4bfc-b446-247edd420e44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSONLファイルのパス\n",
    "file_path = f'{output_path}.jsonl'\n",
    "\n",
    "# JSONオブジェクトを格納するリスト\n",
    "data_list = []\n",
    "\n",
    "# JSONLファイルを読み込んで各行をパース\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    count = 0\n",
    "    for line in file:\n",
    "        # 各行のJSON文字列をPythonの辞書に変換してリストに追加\n",
    "        data_dict = json.loads(line)\n",
    "        # print(data_dict)\n",
    "        data_list.append(data_dict)\n",
    "        print(data_dict[\"instruction\"])\n",
    "        # print(\"[ANSWER]: \" + data_dict[\"cot_answer\"][-100:])\n",
    "        if \"<ANSWER>:\" in data_dict[\"cot_answer\"]:\n",
    "            answer_start = data_dict[\"cot_answer\"].index(\"<ANSWER>:\") + len(\"<ANSWER>:\")\n",
    "            print(\"<ANSWER>:\" + data_dict[\"cot_answer\"][answer_start:])\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        count += 1\n",
    "    print(count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_RAFT_Dataset_Prepare",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "anthropic_env",
   "language": "python",
   "name": "anthropic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
